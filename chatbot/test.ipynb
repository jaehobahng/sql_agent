{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql\n",
      "['click_data', 'purchase_data']\n",
      "[(0.9567846607669617,)]\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv('.env')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"your_database_name\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"your_username\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"your_password\")\n",
    "\n",
    "engine = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n",
    "db = SQLDatabase.from_uri(engine)\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "print(db.run(\"SELECT SUM(PURCHASE_AMT_ACTUAL)/SUM(purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A WHERE EXTRACT(MONTH FROM A.DATE::DATE) = 7 AND EXTRACT(YEAR FROM A.DATE::DATE) = 2024 AND product_id IN (SELECT product_id FROM CLICK_DATA WHERE brand_name = 'Nike');\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = db.run(\"SELECT * FROM PURCHASE_DATA LIMIT 10;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"SELECT SUM(purchase_amt_actual) AS total_purchase_amount\\nFROM purchase_data\\nWHERE date = '6/30/2024'\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "chain = create_sql_query_chain(llm, db)\n",
    "response = chain.invoke({\"question\": \"How much was purchased on 2024-06-30\"})\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[(734.4,)]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.run(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a PostgreSQL expert. Given an input question, first create a syntactically correct PostgreSQL query to run, then look at the results of the query and return the answer to the input question.\n",
      "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per PostgreSQL. You can order the results to return the most informative data in the database.\n",
      "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\n",
      "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
      "Pay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\".\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: Question here\n",
      "SQLQuery: SQL Query to run\n",
      "SQLResult: Result of the SQLQuery\n",
      "Answer: Final answer here\n",
      "\n",
      "Only use the following tables:\n",
      "\u001b[33;1m\u001b[1;3m{table_info}\u001b[0m\n",
      "\n",
      "Question: \u001b[33;1m\u001b[1;3m{input}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "chain.get_prompts()[0].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1810/2892724753.py:3: LangChainDeprecationWarning: The class `QuerySQLDataBaseTool` was deprecated in LangChain 0.3.12 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-community package and should be used instead. To use it run `pip install -U :class:`~langchain-community` and import as `from :class:`~langchain_community.tools import QuerySQLDatabaseTool``.\n",
      "  execute_query = QuerySQLDataBaseTool(db=db)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[(734.4,)]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "\n",
    "execute_query = QuerySQLDataBaseTool(db=db)  # does the db.run part for you\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "chain = write_query | execute_query\n",
    "chain.invoke({\"question\": \"How much was purchased on 2024-06-30\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The total purchase amount on June 30, 2024 was $734.40.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "answer_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Given the following user question, corresponding SQL query, and SQL result, answer the user question.\n",
    "\n",
    "Question: {question}\n",
    "SQL Query: {query}\n",
    "SQL Result: {result}\n",
    "Answer: \"\"\"\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "execute_query = QuerySQLDataBaseTool(db=db)  # does the db.run part for you\n",
    "write_query = create_sql_query_chain(llm, db)\n",
    "\n",
    "answer = answer_prompt | llm | StrOutputParser()\n",
    "chain = (\n",
    "    RunnablePassthrough.assign(query=write_query).assign(result=itemgetter(\"query\") | execute_query) | answer\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"How much was purchased on 2024-06-30\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  query: RunnableAssign(mapper={\n",
       "           input: RunnableLambda(...),\n",
       "           table_info: RunnableLambda(...)\n",
       "         })\n",
       "         | RunnableLambda(lambda x: {k: v for k, v in x.items() if k not in ('question', 'table_names_to_use')})\n",
       "         | PromptTemplate(input_variables=['input', 'table_info'], input_types={}, partial_variables={'top_k': '5'}, template='You are a PostgreSQL expert. Given an input question, first create a syntactically correct PostgreSQL query to run, then look at the results of the query and return the answer to the input question.\\nUnless the user specifies in the question a specific number of examples to obtain, query for at most {top_k} results using the LIMIT clause as per PostgreSQL. You can order the results to return the most informative data in the database.\\nNever query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in double quotes (\") to denote them as delimited identifiers.\\nPay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\\nPay attention to use CURRENT_DATE function to get the current date, if the question involves \"today\".\\n\\nUse the following format:\\n\\nQuestion: Question here\\nSQLQuery: SQL Query to run\\nSQLResult: Result of the SQLQuery\\nAnswer: Final answer here\\n\\nOnly use the following tables:\\n{table_info}\\n\\nQuestion: {input}')\n",
       "         | RunnableBinding(bound=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc0f8995cd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc0f88dd1d0>, root_client=<openai.OpenAI object at 0x7fc0f8b2bd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fc0f88d1b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********')), kwargs={'stop': ['\\nSQLResult:']}, config={}, config_factories=[])\n",
       "         | StrOutputParser()\n",
       "         | RunnableLambda(_strip)\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    result: RunnableLambda(itemgetter('query'))\n",
       "            | QuerySQLDataBaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7fc0fc7b8b10>)\n",
       "  })\n",
       "| PromptTemplate(input_variables=['query', 'question', 'result'], input_types={}, partial_variables={}, template='Given the following user question, corresponding SQL query, and SQL result, answer the user question.\\n\\nQuestion: {question}\\nSQL Query: {query}\\nSQL Result: {result}\\nAnswer: ')\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fc0f8995cd0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fc0f88dd1d0>, root_client=<openai.OpenAI object at 0x7fc0f8b2bd10>, root_async_client=<openai.AsyncOpenAI object at 0x7fc0f88d1b50>, temperature=0.0, model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RunnablePassthrough.assign(query=write_query).assign(result=itemgetter(\"query\") | execute_query) | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import create_sql_query_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv('.env')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"your_database_name\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"your_username\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"your_password\")\n",
    "\n",
    "engine = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n",
    "db = SQLDatabase.from_uri(engine)\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "print(db.run(\"SELECT * FROM PURCHASE_DATA LIMIT 10;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits import create_sql_agent\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "agent_executor = create_sql_agent(llm, db=db, agent_type=\"openai-tools\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_list_tables` with `{}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mclick_data, purchase_data\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_schema` with `{'table_names': 'purchase_data'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3m\n",
      "CREATE TABLE purchase_data (\n",
      "\tpurchase_id BIGINT, \n",
      "\tdate TEXT, \n",
      "\tuser_id BIGINT, \n",
      "\tproduct_id BIGINT, \n",
      "\tpurchase_qty BIGINT, \n",
      "\tpurchase_amt_original BIGINT, \n",
      "\tpurchase_amt_actual DOUBLE PRECISION, \n",
      "\tproduct_actual_price DOUBLE PRECISION, \n",
      "\tproduct_original_price BIGINT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from purchase_data table:\n",
      "purchase_id\tdate\tuser_id\tproduct_id\tpurchase_qty\tpurchase_amt_original\tpurchase_amt_actual\tproduct_actual_price\tproduct_original_price\n",
      "1\t6/30/2024\t78\t2\t2\t918\t734.4\t367.2\t459\n",
      "2\t5/3/2024\t98\t3\t2\t198\t178.2\t89.1\t99\n",
      "3\t5/10/2024\t63\t2\t3\t1377\t1101.6\t367.2\t459\n",
      "*/\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `{'query': 'SELECT product_id, purchase_amt_actual FROM purchase_data ORDER BY purchase_amt_actual DESC LIMIT 10'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[(10, 2411.5), (5, 2020.5), (5, 2020.5), (5, 2020.5), (10, 1929.2), (10, 1929.2), (10, 1929.2), (5, 1616.4), (5, 1616.4), (5, 1616.4)]\u001b[0m\u001b[32;1m\u001b[1;3mThe top 10 products with the highest purchase_amt_actual are as follows:\n",
      "\n",
      "1. Product ID: 10, Purchase Amount Actual: 2411.5\n",
      "2. Product ID: 5, Purchase Amount Actual: 2020.5\n",
      "3. Product ID: 5, Purchase Amount Actual: 2020.5\n",
      "4. Product ID: 5, Purchase Amount Actual: 2020.5\n",
      "5. Product ID: 10, Purchase Amount Actual: 1929.2\n",
      "6. Product ID: 10, Purchase Amount Actual: 1929.2\n",
      "7. Product ID: 10, Purchase Amount Actual: 1929.2\n",
      "8. Product ID: 5, Purchase Amount Actual: 1616.4\n",
      "9. Product ID: 5, Purchase Amount Actual: 1616.4\n",
      "10. Product ID: 5, Purchase Amount Actual: 1616.4\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Top 10 products with the highest purchase_amt_actual?',\n",
       " 'output': 'The top 10 products with the highest purchase_amt_actual are as follows:\\n\\n1. Product ID: 10, Purchase Amount Actual: 2411.5\\n2. Product ID: 5, Purchase Amount Actual: 2020.5\\n3. Product ID: 5, Purchase Amount Actual: 2020.5\\n4. Product ID: 5, Purchase Amount Actual: 2020.5\\n5. Product ID: 10, Purchase Amount Actual: 1929.2\\n6. Product ID: 10, Purchase Amount Actual: 1929.2\\n7. Product ID: 10, Purchase Amount Actual: 1929.2\\n8. Product ID: 5, Purchase Amount Actual: 1616.4\\n9. Product ID: 5, Purchase Amount Actual: 1616.4\\n10. Product ID: 5, Purchase Amount Actual: 1616.4'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"Top 10 products with the highest purchase_amt_actual?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Which brand was sold the most?\",\n",
    "        \"query\": \"SELECT B.BRAND_NAME FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID GROUP BY B.BRAND_NAME ORDER BY SUM(A.PURCHASE_AMT_ACTUAL) DESC LIMIT 1;\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Which category was sold the most?\",\n",
    "        \"query\": \"SELECT B.PRODUCT_CATEGORY FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID GROUP BY B.PRODUCT_CATEGORY ORDER BY SUM(A.PURCHASE_AMT_ACTUAL) DESC LIMIT 1;\",\n",
    "    },    \n",
    "    {\n",
    "        \"input\": \"What is the average discount rate made by nike?\",\n",
    "        \"query\": \"SELECT B.BRAND_NAME, SUM(A.PURCHASE_AMT_ACTUAL)/SUM(A.purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID WHERE B.BRAND_NAME = 'Nike' GROUP BY B.BRAND_NAME;\",\n",
    "    },  \n",
    "    {\n",
    "        \"input\": \"What is the average discount rate made by addidas?\",\n",
    "        \"query\": \"SELECT B.BRAND_NAME, SUM(A.PURCHASE_AMT_ACTUAL)/SUM(A.purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID WHERE B.BRAND_NAME = 'Addidas' GROUP BY B.BRAND_NAME;\",\n",
    "    },  \n",
    "    {\n",
    "        \"input\": \"What is the average discount rate made by nike during july 2024?\",\n",
    "        \"query\": \"SELECT SUM(PURCHASE_AMT_ACTUAL)/SUM(purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A WHERE EXTRACT(MONTH FROM A.DATE::DATE) = 7 AND EXTRACT(YEAR FROM A.DATE::DATE) = 2024 AND product_id IN (SELECT product_id FROM CLICK_DATA WHERE brand_name = 'Nike');\",\n",
    "    },    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "    examples,\n",
    "    OpenAIEmbeddings(),\n",
    "    FAISS,\n",
    "    k=5,\n",
    "    input_keys=[\"input\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    FewShotPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "system_prefix = \"\"\"You are an agent designed to interact with a SQL database.\n",
    "Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top_k} results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "You have access to tools for interacting with the database.\n",
    "Only use the given tools. Only use the information returned by the tools to construct your final answer.\n",
    "You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
    "\n",
    "If the question does not seem related to the database, just return \"I don't know\" as the answer.\n",
    "\n",
    "Here are some examples of user inputs and their corresponding SQL queries:\"\"\"\n",
    "\n",
    "few_shot_prompt = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=PromptTemplate.from_template(\n",
    "        \"User input: {input}\\nSQL query: {query}\"\n",
    "    ),\n",
    "    input_variables=[\"input\", \"dialect\", \"top_k\"],\n",
    "    prefix=system_prefix,\n",
    "    suffix=\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessagePromptTemplate(prompt=few_shot_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "        MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are an agent designed to interact with a SQL database.\n",
      "Given an input question, create a syntactically correct postgres query to run, then look at the results of the query and return the answer.\n",
      "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
      "You can order the results by a relevant column to return the most interesting examples in the database.\n",
      "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
      "You have access to tools for interacting with the database.\n",
      "Only use the given tools. Only use the information returned by the tools to construct your final answer.\n",
      "You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again.\n",
      "\n",
      "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\n",
      "\n",
      "If the question does not seem related to the database, just return \"I don't know\" as the answer.\n",
      "\n",
      "Here are some examples of user inputs and their corresponding SQL queries:\n",
      "\n",
      "User input: Which brand was sold the most?\n",
      "SQL query: SELECT B.BRAND_NAME FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID GROUP BY B.BRAND_NAME ORDER BY SUM(A.PURCHASE_AMT_ACTUAL) DESC LIMIT 1;\n",
      "\n",
      "User input: Which category was sold the most?\n",
      "SQL query: SELECT B.PRODUCT_CATEGORY FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID GROUP BY B.PRODUCT_CATEGORY ORDER BY SUM(A.PURCHASE_AMT_ACTUAL) DESC LIMIT 1;\n",
      "\n",
      "User input: What is the average discount rate made by addidas?\n",
      "SQL query: SELECT B.BRAND_NAME, SUM(A.PURCHASE_AMT_ACTUAL)/SUM(A.purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID WHERE B.BRAND_NAME = 'Addidas' GROUP BY B.BRAND_NAME;\n",
      "\n",
      "User input: What is the average discount rate made by nike?\n",
      "SQL query: SELECT B.BRAND_NAME, SUM(A.PURCHASE_AMT_ACTUAL)/SUM(A.purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A LEFT JOIN CLICK_DATA B ON A.PRODUCT_ID = B.PRODUCT_ID WHERE B.BRAND_NAME = 'Nike' GROUP BY B.BRAND_NAME;\n",
      "\n",
      "User input: What is the average discount rate made by nike during july 2024?\n",
      "SQL query: SELECT SUM(PURCHASE_AMT_ACTUAL)/SUM(purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A WHERE EXTRACT(MONTH FROM A.DATE::DATE) = 7 AND EXTRACT(YEAR FROM A.DATE::DATE) = 2024 AND product_id IN (SELECT product_id FROM CLICK_DATA WHERE brand_name = 'Nike');\n",
      "Human: Which brand was sold the most?\n"
     ]
    }
   ],
   "source": [
    "# Example formatted prompt\n",
    "prompt_val = full_prompt.invoke(\n",
    "    {\n",
    "        \"input\": \"Which brand was sold the most?\",\n",
    "        \"top_k\": 5,\n",
    "        \"dialect\": \"postgres\",\n",
    "        \"agent_scratchpad\": [],\n",
    "    }\n",
    ")\n",
    "print(prompt_val.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_sql_agent(\n",
    "    llm=llm,\n",
    "    db=db,\n",
    "    prompt=full_prompt,\n",
    "    verbose=True,\n",
    "    agent_type=\"openai-tools\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SQL Agent Executor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `sql_db_query` with `{'query': \"SELECT SUM(PURCHASE_AMT_ACTUAL)/SUM(purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A WHERE EXTRACT(MONTH FROM A.DATE::DATE) = 7 AND EXTRACT(YEAR FROM A.DATE::DATE) = 2024 AND product_id IN (SELECT product_id FROM CLICK_DATA WHERE brand_name = 'Nike');\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[(0.9567846607669617,)]\u001b[0m\u001b[32;1m\u001b[1;3mThe average discount rate made by Nike during July 2024 is approximately 95.68%.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the average discount rate made by nike during july 2024?',\n",
       " 'output': 'The average discount rate made by Nike during July 2024 is approximately 95.68%.'}"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"input\": \"What is the average discount rate made by nike during july 2024?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langgraph\n",
    "https://langchain-ai.github.io/langgraph/tutorials/sql-agent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "postgresql\n",
      "['product_data', 'purchase_data']\n",
      "[(0.9567846607669617,)]\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv('.env')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"your_database_name\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"your_username\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"your_password\")\n",
    "\n",
    "engine = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n",
    "db = SQLDatabase.from_uri(engine)\n",
    "print(db.dialect)\n",
    "print(db.get_usable_table_names())\n",
    "print(db.run(\"SELECT SUM(PURCHASE_AMT_ACTUAL)/SUM(purchase_amt_original) AS AVERAGE_DISCOUNT_RATE FROM PURCHASE_DATA A WHERE EXTRACT(MONTH FROM A.DATE::DATE) = 7 AND EXTRACT(YEAR FROM A.DATE::DATE) = 2024 AND product_id IN (SELECT product_id FROM product_data WHERE brand_name = 'Nike');\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")  # Get the error message of current state\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls # Get the tool calls of the last message\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# tools_by_name = {tool.name: tool for tool in tools}\n",
    "# def tool_node(state: dict):\n",
    "#     result = []\n",
    "#     for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "#         tool = tools_by_name[tool_call[\"name\"]]\n",
    "#         observation = tool.invoke(tool_call[\"args\"])\n",
    "#         result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "#     return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "product_data, purchase_data\n",
      "\n",
      "CREATE TABLE purchase_data (\n",
      "\tpurchase_id BIGINT, \n",
      "\tdate TEXT, \n",
      "\tuser_id BIGINT, \n",
      "\tproduct_id BIGINT, \n",
      "\tpurchase_qty BIGINT, \n",
      "\tpurchase_amt_original BIGINT, \n",
      "\tpurchase_amt_actual DOUBLE PRECISION, \n",
      "\tproduct_actual_price DOUBLE PRECISION, \n",
      "\tproduct_original_price BIGINT\n",
      ")\n",
      "\n",
      "/*\n",
      "3 rows from purchase_data table:\n",
      "purchase_id\tdate\tuser_id\tproduct_id\tpurchase_qty\tpurchase_amt_original\tpurchase_amt_actual\tproduct_actual_price\tproduct_original_price\n",
      "1\t6/30/2024\t78\t2\t2\t918\t734.4\t367.2\t459\n",
      "2\t5/3/2024\t98\t3\t2\t198\t178.2\t89.1\t99\n",
      "3\t5/10/2024\t63\t2\t3\t1377\t1101.6\t367.2\t459\n",
      "*/\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(model=\"gpt-4o\"))\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "print(list_tables_tool.invoke(\"\"))\n",
    "print(get_schema_tool.invoke(\"purchase_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[QuerySQLDatabaseTool(description=\"Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql_db_schema to query the correct table fields.\", db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7f091c76ed50>),\n",
       " InfoSQLDatabaseTool(description='Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql_db_list_tables first! Example Input: table1, table2, table3', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7f091c76ed50>),\n",
       " ListSQLDatabaseTool(db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7f091c76ed50>),\n",
       " QuerySQLCheckerTool(description='Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql_db_query!', db=<langchain_community.utilities.sql_database.SQLDatabase object at 0x7f091c76ed50>, llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f091c63b890>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f091c63a2d0>, root_client=<openai.OpenAI object at 0x7f091c63b990>, root_async_client=<openai.AsyncOpenAI object at 0x7f091c70bfd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), llm_chain=LLMChain(verbose=False, prompt=PromptTemplate(input_variables=['dialect', 'query'], input_types={}, partial_variables={}, template='\\n{query}\\nDouble check the {dialect} query above for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nOutput the final SQL query only.\\n\\nSQL Query: '), llm=ChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7f091c63b890>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7f091c63a2d0>, root_client=<openai.OpenAI object at 0x7f091c63b990>, root_async_client=<openai.AsyncOpenAI object at 0x7f091c70bfd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********')), output_parser=StrOutputParser(), llm_kwargs={}))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sql_db_query', 'sql_db_schema', 'sql_db_list_tables', 'sql_db_query_checker']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tool.name for tool in tools]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '6/30/2024', 78, 2, 2, 918, 734.4, 367.2, 459), (2, '5/3/2024', 98, 3, 2, 198, 178.2, 89.1, 99), (3, '5/10/2024', 63, 2, 3, 1377, 1101.6, 367.2, 459), (4, '7/16/2024', 26, 7, 1, 139, 125.1, 125.1, 139), (5, '3/17/2024', 30, 6, 5, 345, 310.5, 62.1, 69), (6, '2/8/2024', 20, 10, 1, 689, 482.3, 482.3, 689), (7, '2/2/2024', 95, 5, 5, 2245, 2020.5, 404.1, 449), (8, '3/1/2024', 7, 5, 3, 1347, 1212.3, 404.1, 449), (9, '7/14/2024', 26, 10, 4, 2756, 1929.2, 482.3, 689), (10, '2/2/2024', 94, 2, 1, 459, 367.2, 367.2, 459)]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run_no_throw(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "print(db_query_tool.invoke(\"SELECT * FROM purchase_data LIMIT 10;\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the postgres query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "You will call the appropriate tool to execute the query after running this check.\"\"\"\n",
    "\n",
    "query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "query_check = query_check_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [db_query_tool], tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "invoke_check = query_check.invoke({\"messages\": [(\"user\", \"SELECT * FROM purchase_data LIMIT 10;\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=[], optional_variables=['messages'], input_types={'messages': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7f091f1e47c0>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'messages': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a SQL expert with a strong attention to detail.\\nDouble check the postgres query for common mistakes, including:\\n- Using NOT IN with NULL values\\n- Using UNION when UNION ALL should have been used\\n- Using BETWEEN for exclusive ranges\\n- Data type mismatch in predicates\\n- Properly quoting identifiers\\n- Using the correct number of arguments for functions\\n- Casting to the correct data type\\n- Using the proper columns for joins\\n\\nIf there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\\n\\nYou will call the appropriate tool to execute the query after running this check.'), additional_kwargs={}), MessagesPlaceholder(variable_name='messages', optional=True)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_check_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_V6F83Y9dJOWzQRxOQaSgrWQy', 'function': {'arguments': '{\"query\":\"SELECT * FROM purchase_data LIMIT 10;\"}', 'name': 'db_query_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 220, 'total_tokens': 244, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f357d293-6c43-47f0-ab15-c4d043e17f83-0', tool_calls=[{'name': 'db_query_tool', 'args': {'query': 'SELECT * FROM purchase_data LIMIT 10;'}, 'id': 'call_V6F83Y9dJOWzQRxOQaSgrWQy', 'type': 'tool_call'}], usage_metadata={'input_tokens': 220, 'output_tokens': 24, 'total_tokens': 244, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'call_V6F83Y9dJOWzQRxOQaSgrWQy',\n",
       "    'function': {'arguments': '{\"query\":\"SELECT * FROM purchase_data LIMIT 10;\"}',\n",
       "     'name': 'db_query_tool'},\n",
       "    'type': 'function'}],\n",
       "  'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 24,\n",
       "   'prompt_tokens': 220,\n",
       "   'total_tokens': 244,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-2024-08-06',\n",
       "  'system_fingerprint': 'fp_d28bcae782',\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'name': None,\n",
       " 'id': 'run-f357d293-6c43-47f0-ab15-c4d043e17f83-0',\n",
       " 'example': False,\n",
       " 'tool_calls': [{'name': 'db_query_tool',\n",
       "   'args': {'query': 'SELECT * FROM purchase_data LIMIT 10;'},\n",
       "   'id': 'call_V6F83Y9dJOWzQRxOQaSgrWQy',\n",
       "   'type': 'tool_call'}],\n",
       " 'invalid_tool_calls': [],\n",
       " 'usage_metadata': {'input_tokens': 220,\n",
       "  'output_tokens': 24,\n",
       "  'total_tokens': 244,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "invoke_check.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "# Define the state for the agent\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "\n",
    "# Add a node for the first tool call\n",
    "def first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    {\n",
    "                        \"name\": \"sql_db_list_tables\",\n",
    "                        \"args\": {},\n",
    "                        \"id\": \"tool_abcd123\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "\n",
    "# Add nodes for the first two tools\n",
    "workflow.add_node(\"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool]))\n",
    "workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "\n",
    "# Add a node for a model to choose the relevant tables based on the question and available tables\n",
    "model_get_schema = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "\n",
    "workflow.add_node(\n",
    "    \"model_get_schema\",\n",
    "    lambda state: {\n",
    "        \"messages\": [model_get_schema.invoke(state[\"messages\"])],\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "# Describe a tool to represent the end state\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Query check function\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the postgres query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "You will call the appropriate tool to execute the query after running this check.\"\"\"\n",
    "\n",
    "query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run_no_throw(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "# print(db_query_tool.invoke(\"SELECT * FROM purchase_data LIMIT 10;\"))\n",
    "\n",
    "\n",
    "query_check = query_check_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [db_query_tool], tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "# invoke_check = query_check.invoke({\"messages\": [(\"user\", \"SELECT * FROM purchase_data LIMIT 10;\")]})\n",
    "\n",
    "\n",
    "def model_check_query(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Use this tool to double-check if your query is correct before executing it.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct postgres query to run, then look at the results of the query and return the answer.\n",
    "\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Output the SQL query that answers the input question without a tool call.\n",
    "\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "\n",
    "If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "If you get an empty result set, you should try to rewrite the query to get a non-empty result set. \n",
    "NEVER make stuff up if you don't have enough information to answer the query... just say you don't have enough information.\n",
    "\n",
    "If you have enough information to answer the input question, simply invoke the appropriate tool to submit the final answer to the user.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_gen_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "# query_gen = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "#     [SubmitFinalAnswer, model_check_query]\n",
    "# )\n",
    "\n",
    "query_gen = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [SubmitFinalAnswer]\n",
    ")\n",
    "\n",
    "def query_gen_node(state: State):\n",
    "    \"\"\"\n",
    "    1. Invoke whatever state the workflow is in now.\n",
    "    2. If tool_calls is not empty, and the tool is not submit final answer, return an error message.\n",
    "    3. If tool_calls is empty return a blank tool message\n",
    "    4. return invoked state + tool message\n",
    "\n",
    "    This LLM generates a SQL query due to system prompt and then either check/runs the query or submits the final answer.\n",
    "    \"\"\"\n",
    "    message = query_gen.invoke(state)\n",
    "\n",
    "    # Sometimes, the LLM will hallucinate and call the wrong tool. We need to catch this and return an error message.\n",
    "    tool_messages = []\n",
    "    if message.tool_calls:\n",
    "        for tc in message.tool_calls:\n",
    "            if tc[\"name\"] != \"SubmitFinalAnswer\":\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n",
    "                        tool_call_id=tc[\"id\"],\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        tool_messages = []\n",
    "    return {\"messages\": [message] + tool_messages}\n",
    "\n",
    "\n",
    "workflow.add_node(\"query_gen\", query_gen_node)\n",
    "\n",
    "# Add a node for the model to check the query before executing it\n",
    "workflow.add_node(\"correct_query\", model_check_query)\n",
    "\n",
    "# Add node for executing the query\n",
    "workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "\n",
    "# Define a conditional edge to decide whether to continue or end the workflow\n",
    "def should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is a tool call, then we finish\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return END\n",
    "    if last_message.content.startswith(\"Error:\"):\n",
    "        return \"query_gen\"\n",
    "    else:\n",
    "        return \"correct_query\"\n",
    "\n",
    "\n",
    "# Specify the edges between the nodes\n",
    "workflow.add_edge(START, \"first_tool_call\")\n",
    "workflow.add_edge(\"first_tool_call\", \"list_tables_tool\")\n",
    "workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_gen\",\n",
    "    should_continue,\n",
    ")\n",
    "workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "# Compile the workflow into a runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a new graph\n",
    "# workflow = StateGraph(State)\n",
    "# workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "# workflow.add_node(\"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool]))\n",
    "# workflow.add_node(\"model_get_schema\", lambda state: {\"messages\": [model_get_schema.invoke(state[\"messages\"])],},) # GPT model\n",
    "# workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "# workflow.add_node(\"query_gen\", query_gen_node) # GPT model\n",
    "# workflow.add_node(\"correct_query\", model_check_query)\n",
    "# workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe a tool to represent the end state\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "oracle_system = \"\"\"\n",
    "You are the oracle, the great AI decision maker.\n",
    "If the user prompt is asking for data, use the list_tables_tool.\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "use only one tool\n",
    "\n",
    "\"\"\"\n",
    "oracle_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_gen_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "oracle_bind = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [list_tables_tool, SubmitFinalAnswer], tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "def run_oracle(state: State):\n",
    "    message = oracle_bind.invoke(state)\n",
    "\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# Define a conditional edge to decide whether to continue or end the workflow\n",
    "def query_or_conv(state: State) -> Literal[END, \"list_tables_tool\"]:\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    # print(state)\n",
    "    tool = messages[-1].additional_kwargs['tool_calls'][0]['function']['name']\n",
    "    # print(tool)\n",
    "    # If there is a tool call, then we finish\n",
    "    if tool == 'SubmitFinalAnswer':\n",
    "        return END\n",
    "    elif tool == 'sql_db_list_tables':\n",
    "        return \"list_tables_tool\"\n",
    "    else:\n",
    "        return \"run_oracle\"\n",
    "    \n",
    "\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "workflow.add_node(\"oracle\", run_oracle)\n",
    "# workflow.add_node(\"final_answer\", final_answer)\n",
    "# workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "workflow.add_node(\"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool]))\n",
    "workflow.add_node(\"model_get_schema\", lambda state: {\"messages\": [model_get_schema.invoke(state[\"messages\"])],},)\n",
    "workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "workflow.add_node(\"query_gen\", query_gen_node)\n",
    "workflow.add_node(\"correct_query\", model_check_query)\n",
    "workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "\n",
    "\n",
    "# Specify the edges between the nodes\n",
    "# workflow.add_edge(START, \"first_tool_call\")\n",
    "workflow.set_entry_point(\"oracle\")\n",
    "\n",
    "workflow.add_conditional_edges(\"oracle\",query_or_conv)\n",
    "\n",
    "# workflow.add_edge(\"oracle\", \"list_tables_tool\")\n",
    "workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"query_gen\",\n",
    "    should_continue, # query_gen, correct_querry, END\n",
    ")\n",
    "workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "# Compile the workflow into a runnable\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='cdabcd29-e2e4-4e00-87bd-a493f06a2203'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_R6eybAdIUt0lCqb62RaF2SRi', 'function': {'arguments': '{\"final_answer\":\"Hello! How can I assist you today with your SQL queries or database questions?\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 356, 'total_tokens': 389, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a0c704e3-7b6c-4cee-b737-010867b1c074-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'Hello! How can I assist you today with your SQL queries or database questions?'}, 'id': 'call_R6eybAdIUt0lCqb62RaF2SRi', 'type': 'tool_call'}], usage_metadata={'input_tokens': 356, 'output_tokens': 33, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi', additional_kwargs={}, response_metadata={}, id='e321c201-a7e7-4e54-8af3-f1f78c3bb379'),\n",
       " AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_6VUPnGJrtU7FizLAXFafVtgJ', 'function': {'arguments': '{\"final_answer\":\"Hello! How can I assist you today with your SQL queries or database questions?\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 356, 'total_tokens': 389, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-cefa7ce6-160c-4163-945f-f23a73a7ef0f-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'Hello! How can I assist you today with your SQL queries or database questions?'}, 'id': 'call_6VUPnGJrtU7FizLAXFafVtgJ', 'type': 'tool_call'}], usage_metadata={'input_tokens': 356, 'output_tokens': 33, 'total_tokens': 389, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'hi'\n",
    "messages = app.invoke({\"messages\": [(\"user\", prompt)]})\n",
    "messages['messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product that sold the most is the \"Puffer Jacket\" with a total quantity of 65 units sold.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = app.invoke(\n",
    "    {\"messages\": [(\"user\", \"What product sold the most?\")]}\n",
    ")\n",
    "json_str = messages[\"messages\"][-1].tool_calls[0][\"args\"][\"final_answer\"]\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What product sold the most?', additional_kwargs={}, response_metadata={}, id='919cafe7-1b8a-4633-956d-7b86583390de'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={}, id='d89dd0c7-5337-4b71-872a-f7294e709c2d', tool_calls=[{'name': 'sql_db_list_tables', 'args': {}, 'id': 'tool_abcd123', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='product_data, purchase_data', name='sql_db_list_tables', id='fca145e6-144e-436f-ba2d-b5d3a98e9393', tool_call_id='tool_abcd123'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_0wCGT2r9emjfwFY67e5frx7y', 'function': {'arguments': '{\"table_names\": \"product_data\"}', 'name': 'sql_db_schema'}, 'type': 'function'}, {'id': 'call_QzY7JcyQEjlTZP2MN0csGb4v', 'function': {'arguments': '{\"table_names\": \"purchase_data\"}', 'name': 'sql_db_schema'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 151, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-d968db20-4f67-4d13-a108-3b3209c325e3-0', tool_calls=[{'name': 'sql_db_schema', 'args': {'table_names': 'product_data'}, 'id': 'call_0wCGT2r9emjfwFY67e5frx7y', 'type': 'tool_call'}, {'name': 'sql_db_schema', 'args': {'table_names': 'purchase_data'}, 'id': 'call_QzY7JcyQEjlTZP2MN0csGb4v', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 51, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='\\nCREATE TABLE product_data (\\n\\tproduct_id BIGINT, \\n\\tproduct_category TEXT, \\n\\tproduct_name TEXT, \\n\\tbrand_name TEXT, \\n\\toriginal_price BIGINT, \\n\\tcurrent_price DOUBLE PRECISION\\n)\\n\\n/*\\n3 rows from product_data table:\\nproduct_id\\tproduct_category\\tproduct_name\\tbrand_name\\toriginal_price\\tcurrent_price\\n1\\tMens Pants\\tBaloon Pants\\tNike\\t149\\t104.3\\n2\\tMens Outer\\tPuffer Jacket\\tAddidas\\t459\\t367.2\\n3\\tMens Top\\tT-shirt\\tUnder Armour\\t99\\t89.1\\n*/', name='sql_db_schema', id='312e6abb-df40-4325-a305-95a14ca221b0', tool_call_id='call_0wCGT2r9emjfwFY67e5frx7y'),\n",
       "  ToolMessage(content='\\nCREATE TABLE purchase_data (\\n\\tpurchase_id BIGINT, \\n\\tdate TEXT, \\n\\tuser_id BIGINT, \\n\\tproduct_id BIGINT, \\n\\tpurchase_qty BIGINT, \\n\\tpurchase_amt_original BIGINT, \\n\\tpurchase_amt_actual DOUBLE PRECISION, \\n\\tproduct_actual_price DOUBLE PRECISION, \\n\\tproduct_original_price BIGINT\\n)\\n\\n/*\\n3 rows from purchase_data table:\\npurchase_id\\tdate\\tuser_id\\tproduct_id\\tpurchase_qty\\tpurchase_amt_original\\tpurchase_amt_actual\\tproduct_actual_price\\tproduct_original_price\\n1\\t6/30/2024\\t78\\t2\\t2\\t918\\t734.4\\t367.2\\t459\\n2\\t5/3/2024\\t98\\t3\\t2\\t198\\t178.2\\t89.1\\t99\\n3\\t5/10/2024\\t63\\t2\\t3\\t1377\\t1101.6\\t367.2\\t459\\n*/', name='sql_db_schema', id='78ae7fa1-c8bc-4762-9fe5-44577040b5bc', tool_call_id='call_QzY7JcyQEjlTZP2MN0csGb4v'),\n",
       "  AIMessage(content=\"To determine which product sold the most, we need to sum the `purchase_qty` for each `product_id` in the `purchase_data` table and then join it with the `product_data` table to get the product name. Let's write the query to find this information.\\n\\n```sql\\nSELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;\\n```\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 754, 'total_tokens': 883, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'stop', 'logprobs': None}, id='run-f86d8d8e-2a0f-40e1-9ba3-c6de88e6b3c0-0', usage_metadata={'input_tokens': 754, 'output_tokens': 129, 'total_tokens': 883, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ibd43E418mqCcF9YrBwT5cvz', 'function': {'arguments': '{\"query\":\"SELECT \\\\n    pd.product_name, \\\\n    SUM(pch.purchase_qty) AS total_quantity_sold\\\\nFROM \\\\n    purchase_data pch\\\\nJOIN \\\\n    product_data pd ON pch.product_id = pd.product_id\\\\nGROUP BY \\\\n    pd.product_name\\\\nORDER BY \\\\n    total_quantity_sold DESC\\\\nLIMIT 1;\"}', 'name': 'db_query_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 338, 'total_tokens': 426, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-bce8cb1c-272e-41d4-aec2-33b431bdcddc-0', tool_calls=[{'name': 'db_query_tool', 'args': {'query': 'SELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;'}, 'id': 'call_ibd43E418mqCcF9YrBwT5cvz', 'type': 'tool_call'}], usage_metadata={'input_tokens': 338, 'output_tokens': 88, 'total_tokens': 426, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"[('Puffer Jacket', Decimal('65'))]\", name='db_query_tool', id='30103741-58a5-4235-9789-7953068c7749', tool_call_id='call_ibd43E418mqCcF9YrBwT5cvz'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_QPCp2tox9iO0HAX5plRrvfj4', 'function': {'arguments': '{\"final_answer\":\"The product that sold the most is the \\\\\"Puffer Jacket\\\\\" with a total quantity of 65 units sold.\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 40, 'prompt_tokens': 992, 'total_tokens': 1032, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_9faba9f038', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eaff034f-d94a-4268-88cd-a77b695da0a0-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'The product that sold the most is the \"Puffer Jacket\" with a total quantity of 65 units sold.'}, 'id': 'call_QPCp2tox9iO0HAX5plRrvfj4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 992, 'output_tokens': 40, 'total_tokens': 1032, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': \"To determine which product sold the most, we need to sum the `purchase_qty` for each `product_id` in the `purchase_data` table and then join it with the `product_data` table to get the product name. Let's write the query to find this information.\\n\\n```sql\\nSELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;\\n```\",\n",
       " 'additional_kwargs': {'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 129,\n",
       "   'prompt_tokens': 754,\n",
       "   'total_tokens': 883,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-2024-08-06',\n",
       "  'system_fingerprint': 'fp_9faba9f038',\n",
       "  'finish_reason': 'stop',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'id': 'run-f86d8d8e-2a0f-40e1-9ba3-c6de88e6b3c0-0',\n",
       " 'usage_metadata': {'input_tokens': 754,\n",
       "  'output_tokens': 129,\n",
       "  'total_tokens': 883,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
       " 'tool_calls': [],\n",
       " 'invalid_tool_calls': []}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['messages'][-4].to_json()['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '',\n",
       " 'additional_kwargs': {'tool_calls': [{'id': 'call_ibd43E418mqCcF9YrBwT5cvz',\n",
       "    'function': {'arguments': '{\"query\":\"SELECT \\\\n    pd.product_name, \\\\n    SUM(pch.purchase_qty) AS total_quantity_sold\\\\nFROM \\\\n    purchase_data pch\\\\nJOIN \\\\n    product_data pd ON pch.product_id = pd.product_id\\\\nGROUP BY \\\\n    pd.product_name\\\\nORDER BY \\\\n    total_quantity_sold DESC\\\\nLIMIT 1;\"}',\n",
       "     'name': 'db_query_tool'},\n",
       "    'type': 'function'}],\n",
       "  'refusal': None},\n",
       " 'response_metadata': {'token_usage': {'completion_tokens': 88,\n",
       "   'prompt_tokens': 338,\n",
       "   'total_tokens': 426,\n",
       "   'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
       "    'audio_tokens': 0,\n",
       "    'reasoning_tokens': 0,\n",
       "    'rejected_prediction_tokens': 0},\n",
       "   'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}},\n",
       "  'model_name': 'gpt-4o-2024-08-06',\n",
       "  'system_fingerprint': 'fp_d28bcae782',\n",
       "  'finish_reason': 'tool_calls',\n",
       "  'logprobs': None},\n",
       " 'type': 'ai',\n",
       " 'id': 'run-bce8cb1c-272e-41d4-aec2-33b431bdcddc-0',\n",
       " 'tool_calls': [{'name': 'db_query_tool',\n",
       "   'args': {'query': 'SELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;'},\n",
       "   'id': 'call_ibd43E418mqCcF9YrBwT5cvz',\n",
       "   'type': 'tool_call'}],\n",
       " 'usage_metadata': {'input_tokens': 338,\n",
       "  'output_tokens': 88,\n",
       "  'total_tokens': 426,\n",
       "  'input_token_details': {'audio': 0, 'cache_read': 0},\n",
       "  'output_token_details': {'audio': 0, 'reasoning': 0}},\n",
       " 'invalid_tool_calls': []}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages['messages'][-3].to_json()['kwargs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What product sold the most?', additional_kwargs={}, response_metadata={}, id='2b98ca09-9e38-4e31-afce-7b989e0907db'),\n",
       "  AIMessage(content='', additional_kwargs={}, response_metadata={}, id='df712d24-0d16-4300-8230-be81a76548dc', tool_calls=[{'name': 'sql_db_list_tables', 'args': {}, 'id': 'tool_abcd123', 'type': 'tool_call'}]),\n",
       "  ToolMessage(content='product_data, purchase_data', name='sql_db_list_tables', id='862a3012-3ef5-4093-b04c-8c95d60ebeb0', tool_call_id='tool_abcd123'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_clKf4NawseN1yiZo0CztSrlf', 'function': {'arguments': '{\"table_names\": \"product_data\"}', 'name': 'sql_db_schema'}, 'type': 'function'}, {'id': 'call_oBZcvRoeHKTawrVw0hYWjCHL', 'function': {'arguments': '{\"table_names\": \"purchase_data\"}', 'name': 'sql_db_schema'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 151, 'total_tokens': 202, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-7562925c-9b1a-4bec-afcf-68c2eb73018c-0', tool_calls=[{'name': 'sql_db_schema', 'args': {'table_names': 'product_data'}, 'id': 'call_clKf4NawseN1yiZo0CztSrlf', 'type': 'tool_call'}, {'name': 'sql_db_schema', 'args': {'table_names': 'purchase_data'}, 'id': 'call_oBZcvRoeHKTawrVw0hYWjCHL', 'type': 'tool_call'}], usage_metadata={'input_tokens': 151, 'output_tokens': 51, 'total_tokens': 202, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='\\nCREATE TABLE product_data (\\n\\tproduct_id BIGINT, \\n\\tproduct_category TEXT, \\n\\tproduct_name TEXT, \\n\\tbrand_name TEXT, \\n\\toriginal_price BIGINT, \\n\\tcurrent_price DOUBLE PRECISION\\n)\\n\\n/*\\n3 rows from product_data table:\\nproduct_id\\tproduct_category\\tproduct_name\\tbrand_name\\toriginal_price\\tcurrent_price\\n1\\tMens Pants\\tBaloon Pants\\tNike\\t149\\t104.3\\n2\\tMens Outer\\tPuffer Jacket\\tAddidas\\t459\\t367.2\\n3\\tMens Top\\tT-shirt\\tUnder Armour\\t99\\t89.1\\n*/', name='sql_db_schema', id='4f1315cb-70cb-44a4-a6de-7b4afdb5ff99', tool_call_id='call_clKf4NawseN1yiZo0CztSrlf'),\n",
       "  ToolMessage(content='\\nCREATE TABLE purchase_data (\\n\\tpurchase_id BIGINT, \\n\\tdate TEXT, \\n\\tuser_id BIGINT, \\n\\tproduct_id BIGINT, \\n\\tpurchase_qty BIGINT, \\n\\tpurchase_amt_original BIGINT, \\n\\tpurchase_amt_actual DOUBLE PRECISION, \\n\\tproduct_actual_price DOUBLE PRECISION, \\n\\tproduct_original_price BIGINT\\n)\\n\\n/*\\n3 rows from purchase_data table:\\npurchase_id\\tdate\\tuser_id\\tproduct_id\\tpurchase_qty\\tpurchase_amt_original\\tpurchase_amt_actual\\tproduct_actual_price\\tproduct_original_price\\n1\\t6/30/2024\\t78\\t2\\t2\\t918\\t734.4\\t367.2\\t459\\n2\\t5/3/2024\\t98\\t3\\t2\\t198\\t178.2\\t89.1\\t99\\n3\\t5/10/2024\\t63\\t2\\t3\\t1377\\t1101.6\\t367.2\\t459\\n*/', name='sql_db_schema', id='1d9b7adc-e56f-4c1c-ae84-5a6d2744d0f2', tool_call_id='call_oBZcvRoeHKTawrVw0hYWjCHL'),\n",
       "  AIMessage(content=\"To determine which product sold the most, we need to sum the `purchase_qty` for each `product_id` in the `purchase_data` table and then join it with the `product_data` table to get the product name. Let's write the query to find this information.\\n\\n```sql\\nSELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;\\n```\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 3717, 'total_tokens': 3846, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'stop', 'logprobs': None}, id='run-7d1b76cb-299e-4528-b335-42e93a46a2d2-0', usage_metadata={'input_tokens': 3717, 'output_tokens': 129, 'total_tokens': 3846, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TUu3KBuWyt7Oj6hvScQVxChe', 'function': {'arguments': '{\"query\":\"SELECT \\\\n    pd.product_name, \\\\n    SUM(pch.purchase_qty) AS total_quantity_sold\\\\nFROM \\\\n    purchase_data pch\\\\nJOIN \\\\n    product_data pd ON pch.product_id = pd.product_id\\\\nGROUP BY \\\\n    pd.product_name\\\\nORDER BY \\\\n    total_quantity_sold DESC\\\\nLIMIT 1;\"}', 'name': 'db_query_tool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 338, 'total_tokens': 426, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-fd087821-ee7c-4f7a-b95c-43fc810266c1-0', tool_calls=[{'name': 'db_query_tool', 'args': {'query': 'SELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;'}, 'id': 'call_TUu3KBuWyt7Oj6hvScQVxChe', 'type': 'tool_call'}], usage_metadata={'input_tokens': 338, 'output_tokens': 88, 'total_tokens': 426, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content=\"[('Puffer Jacket', Decimal('65'))]\", name='db_query_tool', id='283cb3ef-a4ae-49e5-9710-db7f5ff6584c', tool_call_id='call_TUu3KBuWyt7Oj6hvScQVxChe'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_nVrnVvdtkVN0S4Yi75JUflHw', 'function': {'arguments': '{\"final_answer\":\"The product that sold the most is the \\\\\"Puffer Jacket\\\\\" with a total quantity of 65 sold.\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 39, 'prompt_tokens': 3955, 'total_tokens': 3994, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 3712}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_d28bcae782', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-3feede22-39ee-4262-b4c6-53f44227d3be-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'The product that sold the most is the \"Puffer Jacket\" with a total quantity of 65 sold.'}, 'id': 'call_nVrnVvdtkVN0S4Yi75JUflHw', 'type': 'tool_call'}], usage_metadata={'input_tokens': 3955, 'output_tokens': 39, 'total_tokens': 3994, 'input_token_details': {'audio': 0, 'cache_read': 3712}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for event in app.stream(\n",
    "#     {\"messages\": [(\"user\", \"What product sold the most?\")]}\n",
    "# ):\n",
    "#     print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =[\n",
    "         AIMessage(content='', additional_kwargs={}, response_metadata={}, id='df712d24-0d16-4300-8230-be81a76548dc', tool_calls=[{'name': 'sql_db_list_tables', 'args': {}, 'id': 'tool_abcd123', 'type': 'tool_call'}]),\n",
    "         ToolMessage(content='product_data, purchase_data', name='sql_db_list_tables', id='862a3012-3ef5-4093-b04c-8c95d60ebeb0', tool_call_id='tool_abcd123')\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a node for a model to choose the relevant tables based on the question and available tables\n",
    "model_get_schema = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "\n",
    "a = model_get_schema.invoke(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'sql_db_schema',\n",
       "  'args': {'table_names': 'product_data'},\n",
       "  'id': 'call_UHonQJ1alrX6ha1hNUgLbkPo',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'sql_db_schema',\n",
       "  'args': {'table_names': 'purchase_data'},\n",
       "  'id': 'call_7U9lfelmuAwA45A1zRWQPi8S',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iMPORT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/app\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir('/app')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lang_model import chatbot\n",
    "model = chatbot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product that sold the most is the \"Puffer Jacket\" with a total quantity sold of 65 units.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = model.invoke(\n",
    "    {\"messages\": [(\"user\", \"What product sold the most?\")]}\n",
    ")\n",
    "\n",
    "json_str = messages[\"messages\"][-1].tool_calls[0][\"args\"][\"final_answer\"]\n",
    "json_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Iterate through the results and get the code snippet for each slice where 'db_query_tool' appears\n",
    "query = []\n",
    "\n",
    "for i, item in enumerate(messages[\"messages\"]):\n",
    "    try:\n",
    "        tool_name = item.additional_kwargs[\"tool_calls\"][0][\"function\"][\"name\"]\n",
    "        if tool_name == \"db_query_tool\":\n",
    "            a = item.additional_kwargs['tool_calls'][0]['function']['arguments']\n",
    "            actual_dict = json.loads(a)\n",
    "            query.append(actual_dict['query'])\n",
    "            \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "query[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT \n",
      "    pd.product_name, \n",
      "    SUM(pch.purchase_qty) AS total_quantity_sold\n",
      "FROM \n",
      "    purchase_data pch\n",
      "JOIN \n",
      "    product_data pd ON pch.product_id = pd.product_id\n",
      "GROUP BY \n",
      "    pd.product_name\n",
      "ORDER BY \n",
      "    total_quantity_sold DESC\n",
      "LIMIT 1;\n"
     ]
    }
   ],
   "source": [
    "print(query[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT \\n    pd.product_name, \\n    SUM(pch.purchase_qty) AS total_quantity_sold\\nFROM \\n    purchase_data pch\\nJOIN \\n    product_data pd ON pch.product_id = pd.product_id\\nGROUP BY \\n    pd.product_name\\nORDER BY \\n    total_quantity_sold DESC\\nLIMIT 1;\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "from langchain_community.utilities import SQLDatabase\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv('.env')\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Database connection details\n",
    "DB_HOST = os.getenv(\"DB_HOST\", \"localhost\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\", \"5432\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\", \"your_database_name\")\n",
    "DB_USER = os.getenv(\"DB_USER\", \"your_username\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\", \"your_password\")\n",
    "\n",
    "engine = f'postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}'\n",
    "\n",
    "db = SQLDatabase.from_uri(engine)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda, RunnableWithFallbacks\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> RunnableWithFallbacks[Any, dict]:\n",
    "    \"\"\"\n",
    "    Create a ToolNode with a fallback to handle errors and surface them to the agent.\n",
    "    \"\"\"\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)], exception_key=\"error\"\n",
    "    )\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    error = state.get(\"error\")  # Get the error message of current state\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls # Get the tool calls of the last message\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",\n",
    "                tool_call_id=tc[\"id\"],\n",
    "            )\n",
    "            for tc in tool_calls\n",
    "        ]\n",
    "    }\n",
    "\n",
    "\n",
    "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=ChatOpenAI(model=\"gpt-4o\"))\n",
    "tools = toolkit.get_tools()\n",
    "\n",
    "list_tables_tool = next(tool for tool in tools if tool.name == \"sql_db_list_tables\")\n",
    "get_schema_tool = next(tool for tool in tools if tool.name == \"sql_db_schema\")\n",
    "\n",
    "\n",
    "# print(list_tables_tool.invoke(\"\"))\n",
    "# print(get_schema_tool.invoke(\"purchase_data\"))\n",
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run_no_throw(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "# print(db_query_tool.invoke(\"SELECT * FROM purchase_data LIMIT 10;\"))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from typing import Annotated, Literal\n",
    "\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "\n",
    "\n",
    "# Define the state for the agent\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "\n",
    "\n",
    "# Add a node for the first tool call\n",
    "def first_tool_call(state: State) -> dict[str, list[AIMessage]]:\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            AIMessage(\n",
    "                content=\"\",\n",
    "                tool_calls=[\n",
    "                    {\n",
    "                        \"name\": \"sql_db_list_tables\",\n",
    "                        \"args\": {},\n",
    "                        \"id\": \"tool_abcd123\",\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Add a node for a model to choose the relevant tables based on the question and available tables\n",
    "model_get_schema = ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [get_schema_tool]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Describe a tool to represent the end state\n",
    "class SubmitFinalAnswer(BaseModel):\n",
    "    \"\"\"Submit the final answer to the user based on the query results.\"\"\"\n",
    "\n",
    "    final_answer: str = Field(..., description=\"The final answer to the user\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Query check function\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "query_check_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "Double check the postgres query for common mistakes, including:\n",
    "- Using NOT IN with NULL values\n",
    "- Using UNION when UNION ALL should have been used\n",
    "- Using BETWEEN for exclusive ranges\n",
    "- Data type mismatch in predicates\n",
    "- Properly quoting identifiers\n",
    "- Using the correct number of arguments for functions\n",
    "- Casting to the correct data type\n",
    "- Using the proper columns for joins\n",
    "\n",
    "If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query.\n",
    "\n",
    "You will call the appropriate tool to execute the query after running this check.\"\"\"\n",
    "\n",
    "query_check_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_check_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def db_query_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Execute a SQL query against the database and get back the result.\n",
    "    If the query is not correct, an error message will be returned.\n",
    "    If an error is returned, rewrite the query, check the query, and try again.\n",
    "    \"\"\"\n",
    "    result = db.run_no_throw(query)\n",
    "    if not result:\n",
    "        return \"Error: Query failed. Please rewrite your query and try again.\"\n",
    "    return result\n",
    "\n",
    "\n",
    "# print(db_query_tool.invoke(\"SELECT * FROM purchase_data LIMIT 10;\"))\n",
    "\n",
    "\n",
    "query_check = query_check_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [db_query_tool], tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "# invoke_check = query_check.invoke({\"messages\": [(\"user\", \"SELECT * FROM purchase_data LIMIT 10;\")]})\n",
    "\n",
    "def model_check_query(state: State) -> dict[str, list[AIMessage]]:\n",
    "    \"\"\"\n",
    "    Use this tool to double-check if your query is correct before executing it.\n",
    "    \"\"\"\n",
    "    return {\"messages\": [query_check.invoke({\"messages\": [state[\"messages\"][-1]]})]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct postgres query to run, then look at the results of the query and return the answer.\n",
    "If the user's input is not asking for data, do not generate a query, SubmitFinalAnswer and ask for clarification.\n",
    "\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\n",
    "When generating the query:\n",
    "\n",
    "Output the SQL query that answers the input question without a tool call.\n",
    "\n",
    "Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most 5 results.\n",
    "You can order the results by a relevant column to return the most interesting examples in the database.\n",
    "Never query for all the columns from a specific table, only ask for the relevant columns given the question.\n",
    "\n",
    "If you get an error while executing a query, rewrite the query and try again.\n",
    "\n",
    "NEVER make stuff up if you don't have enough information to answer the query... just say you don't have enough information.\n",
    "\n",
    "If you have enough information to answer the input question, simply invoke the appropriate tool to submit the final answer to the user.\n",
    "\n",
    "DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database.\"\"\"\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_gen_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "query_gen = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [SubmitFinalAnswer, model_check_query]\n",
    ")\n",
    "\n",
    "\n",
    "def query_gen_node(state: State):\n",
    "    \"\"\"\n",
    "    1. Invoke whatever state the workflow is in now.\n",
    "    2. If tool_calls is not empty, and the tool is not submit final answer, return an error message.\n",
    "    3. If tool_calls is empty return a blank tool message\n",
    "    4. return invoked state + tool message\n",
    "\n",
    "    This LLM generates a SQL query due to system prompt and then either check/runs the query or submits the final answer.\n",
    "    \"\"\"\n",
    "    message = query_gen.invoke(state)\n",
    "\n",
    "    # Sometimes, the LLM will hallucinate and call the wrong tool. We need to catch this and return an error message.\n",
    "    tool_messages = []\n",
    "    if message.tool_calls:\n",
    "        for tc in message.tool_calls:\n",
    "            if tc[\"name\"] != \"SubmitFinalAnswer\":\n",
    "                tool_messages.append(\n",
    "                    ToolMessage(\n",
    "                        content=f\"Error: The wrong tool was called: {tc['name']}. Please fix your mistakes. Remember to only call SubmitFinalAnswer to submit the final answer. Generated queries should be outputted WITHOUT a tool call.\",\n",
    "                        tool_call_id=tc[\"id\"],\n",
    "                    )\n",
    "                )\n",
    "    else:\n",
    "        tool_messages = []\n",
    "    return {\"messages\": [message] + tool_messages}\n",
    "\n",
    "\n",
    "\n",
    "# Define a conditional edge to decide whether to continue or end the workflow\n",
    "def should_continue(state: State) -> Literal[END, \"correct_query\", \"query_gen\"]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is a tool call, then we finish\n",
    "    if getattr(last_message, \"tool_calls\", None):\n",
    "        return END\n",
    "    if last_message.content.startswith(\"Error:\"):\n",
    "        return \"query_gen\"\n",
    "    else:\n",
    "        return \"correct_query\"\n",
    "\n",
    "\n",
    "import uuid\n",
    "\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "\n",
    "chats_by_session_id = {}\n",
    "\n",
    "\n",
    "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
    "    chat_history = chats_by_session_id.get(session_id)\n",
    "    if chat_history is None:\n",
    "        chat_history = InMemoryChatMessageHistory()\n",
    "        chats_by_session_id[session_id] = chat_history\n",
    "    return chat_history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "oracle_system = \"\"\"\n",
    "You are the oracle, the great AI decision maker.\n",
    "If the user prompt is asking for data, use the list_tables_tool.\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "use only one tool\n",
    "\n",
    "\"\"\"\n",
    "oracle_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", query_gen_system), \n",
    "    (\"placeholder\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "oracle_bind = oracle_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [list_tables_tool, SubmitFinalAnswer], tool_choice=\"required\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def run_oracle(state: State, config):\n",
    "    message = oracle_bind.invoke(state)\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "\n",
    "# Define a conditional edge to decide whether to continue or end the workflow\n",
    "def query_or_conv(state: State) -> Literal[END, \"list_tables_tool\"]:\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    # print(state)\n",
    "    tool = messages[-1].additional_kwargs['tool_calls'][0]['function']['name']\n",
    "    # print(tool)\n",
    "    # If there is a tool call, then we finish\n",
    "    if tool == 'SubmitFinalAnswer':\n",
    "        return END\n",
    "    elif tool == 'sql_db_list_tables':\n",
    "        return \"list_tables_tool\"\n",
    "    else:\n",
    "        return \"run_oracle\"\n",
    "    \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "\n",
    "\n",
    "def chatbot():\n",
    "\n",
    "    workflow = StateGraph(State)\n",
    "    workflow.add_node(\"oracle\", run_oracle)\n",
    "    # workflow.add_node(\"final_answer\", final_answer)\n",
    "    # workflow.add_node(\"first_tool_call\", first_tool_call)\n",
    "    workflow.add_node(\"list_tables_tool\", create_tool_node_with_fallback([list_tables_tool]))\n",
    "    workflow.add_node(\"model_get_schema\", lambda state: {\"messages\": [model_get_schema.invoke(state[\"messages\"])],},)\n",
    "    workflow.add_node(\"get_schema_tool\", create_tool_node_with_fallback([get_schema_tool]))\n",
    "    workflow.add_node(\"query_gen\", query_gen_node)\n",
    "    workflow.add_node(\"correct_query\", model_check_query)\n",
    "    workflow.add_node(\"execute_query\", create_tool_node_with_fallback([db_query_tool]))\n",
    "\n",
    "\n",
    "\n",
    "    # Specify the edges between the nodes\n",
    "    # workflow.add_edge(START, \"first_tool_call\")\n",
    "    workflow.set_entry_point(\"oracle\")\n",
    "\n",
    "    workflow.add_conditional_edges(\"oracle\",query_or_conv)\n",
    "\n",
    "    # workflow.add_edge(\"oracle\", \"list_tables_tool\")\n",
    "    workflow.add_edge(\"list_tables_tool\", \"model_get_schema\")\n",
    "    workflow.add_edge(\"model_get_schema\", \"get_schema_tool\")\n",
    "    workflow.add_edge(\"get_schema_tool\", \"query_gen\")\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"query_gen\",\n",
    "        should_continue, # query_gen, correct_querry, END\n",
    "    )\n",
    "    workflow.add_edge(\"correct_query\", \"execute_query\")\n",
    "    workflow.add_edge(\"execute_query\", \"query_gen\")\n",
    "\n",
    "    # Compile the workflow into a runnable\n",
    "    app = workflow.compile(checkpointer=memory)\n",
    "\n",
    "    return app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = chatbot()\n",
    "\n",
    "import uuid\n",
    "\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "# session_id = uuid.uuid4()\n",
    "# config = {\"configurable\": {\"session_id\": session_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='my name is jaeho', additional_kwargs={}, response_metadata={}, id='431f99f8-c7af-4506-9dc5-d10175cb663a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IwsQFsOzzSLwKJ6gdPy88VMb', 'function': {'arguments': '{\"final_answer\":\"Hello Jaeho! How can I assist you today?\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 361, 'total_tokens': 390, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a6eb32b6-5462-4489-8ad7-28c8a46dd7c9-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'Hello Jaeho! How can I assist you today?'}, 'id': 'call_IwsQFsOzzSLwKJ6gdPy88VMb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 361, 'output_tokens': 29, 'total_tokens': 390, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = 'my name is jaeho'\n",
    "messages = model.invoke({\"messages\": [(\"user\", prompt)]}, config=config)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Pregel.get_state_history at 0x7f7b7ea7a340>"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_state_history(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='my name is jaeho', additional_kwargs={}, response_metadata={}, id='431f99f8-c7af-4506-9dc5-d10175cb663a'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_IwsQFsOzzSLwKJ6gdPy88VMb', 'function': {'arguments': '{\"final_answer\":\"Hello Jaeho! How can I assist you today?\"}', 'name': 'SubmitFinalAnswer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 361, 'total_tokens': 390, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-a6eb32b6-5462-4489-8ad7-28c8a46dd7c9-0', tool_calls=[{'name': 'SubmitFinalAnswer', 'args': {'final_answer': 'Hello Jaeho! How can I assist you today?'}, 'id': 'call_IwsQFsOzzSLwKJ6gdPy88VMb', 'type': 'tool_call'}], usage_metadata={'input_tokens': 361, 'output_tokens': 29, 'total_tokens': 390, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_RELk48icpkqCu5TH9E0cw78x\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[238], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwhat is my name?\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m messages \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m messages\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1936\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1934\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1935\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1936\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1938\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1656\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1652\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels):\n\u001b[0;32m-> 1656\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1657\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1658\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1659\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1660\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1663\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/runner.py:167\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m    165\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_SEND\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/pregel/retry.py:40\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     38\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     42\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py:408\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    404\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[1;32m    405\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    406\u001b[0m )\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 408\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[235], line 314\u001b[0m, in \u001b[0;36mrun_oracle\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_oracle\u001b[39m(state: State, config):\n\u001b[0;32m--> 314\u001b[0m     message \u001b[38;5;241m=\u001b[39m \u001b[43moracle_bind\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [message]}\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3026\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/runnables/base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:717\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 717\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"An assistant message with 'tool_calls' must be followed by tool messages responding to each 'tool_call_id'. The following tool_call_ids did not have response messages: call_RELk48icpkqCu5TH9E0cw78x\", 'type': 'invalid_request_error', 'param': 'messages.[3].role', 'code': None}}",
      "\u001b[0mDuring task with name 'oracle' and id '8d5435cb-5ae7-080c-4da0-c154b957a964'"
     ]
    }
   ],
   "source": [
    "prompt = 'what is my name?'\n",
    "messages = model.invoke({\"messages\": [(\"user\", prompt)]}, config=config)\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(\"final_answer\")\n",
    "def final_answer(summary: str):\n",
    "    \"\"\"\n",
    "    Create a simple response for the user input based on memory\n",
    "    \"\"\"\n",
    "    return \"\"\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "oracle_system = \"\"\"\n",
    "You are the oracle, the great AI decision maker.\n",
    "If the user prompt is asking for data, use the list_tables_tool.\n",
    "If the user is engaging in a conversation, use final_answer.\n",
    "use only one tool\n",
    "\n",
    "\"\"\"\n",
    "oracle_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", query_gen_system), (\"placeholder\", \"{messages}\")]\n",
    ")\n",
    "\n",
    "oracle = query_gen_prompt | ChatOpenAI(model=\"gpt-4o\", temperature=0).bind_tools(\n",
    "    [list_tables_tool, final_answer], tool_choice=\"required\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = oracle.invoke({\"messages\": [(\"user\", \"Hi\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = oracle.invoke({\"messages\": [(\"user\", \"What product was sold the most?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_answer\n",
      "sql_db_list_tables\n"
     ]
    }
   ],
   "source": [
    "print(c.tool_calls[0]['name'])\n",
    "print(q.tool_calls[0]['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KVWcT70rOck1GsgPz97FkaQm', 'function': {'arguments': '{\"summary\":\"Hello! How can I assist you today?\"}', 'name': 'final_answer'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 345, 'total_tokens': 368, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_5f20662549', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-1904c727-1abc-466d-a0c7-6fb4befb68a5-0', tool_calls=[{'name': 'final_answer', 'args': {'summary': 'Hello! How can I assist you today?'}, 'id': 'call_KVWcT70rOck1GsgPz97FkaQm', 'type': 'tool_call'}], usage_metadata={'input_tokens': 345, 'output_tokens': 23, 'total_tokens': 368, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
